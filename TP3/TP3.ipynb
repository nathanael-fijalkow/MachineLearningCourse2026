{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d8e3e2",
   "metadata": {},
   "source": [
    "# Lab 3: Validation Curves and Learning Curves\n",
    "\n",
    "In this lab, you will learn how to use **validation curves** and **learning curves** to:\n",
    "- Choose the best hyperparameters for a model\n",
    "- Determine whether adding more training samples would improve model performance\n",
    "- Diagnose overfitting and underfitting\n",
    "\n",
    "**Objectives:**\n",
    "- Understand the difference between validation curves and learning curves\n",
    "- Use validation curves to optimize hyperparameters\n",
    "- Use learning curves to assess data needs\n",
    "- Interpret curves to make informed decisions about model complexity and data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cf4ad",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import (\n",
    "    ValidationCurveDisplay, \n",
    "    LearningCurveDisplay,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf894b84",
   "metadata": {},
   "source": [
    "We will use the **digits dataset** from scikit-learn, which contains 8x8 images of handwritten digits (0-9). This is a classic machine learning dataset with 1,797 samples and 64 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bba613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53a81c",
   "metadata": {},
   "source": [
    "## 2. Understanding Validation Curves\n",
    "\n",
    "A **validation curve** shows how a model's performance varies as we change a single hyperparameter.\n",
    "\n",
    "The curve plots:\n",
    "- **Training score** (blue): Performance on the training data\n",
    "- **Validation score** (orange): Performance on validation folds (via cross-validation)\n",
    "\n",
    "This helps us:\n",
    "1. **Identify underfitting**: When both training and validation scores are low and similar\n",
    "2. **Identify overfitting**: When training score is high but validation score is low (large gap)\n",
    "3. **Find the sweet spot**: Where the validation score is highest (best generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb97a8",
   "metadata": {},
   "source": [
    "### 2.1 Validation Curve for k-Nearest Neighbors (k parameter)\n",
    "\n",
    "In KNN, the `n_neighbors` parameter (k) controls model complexity:\n",
    "- **Small k** (e.g., k=1): Model is very flexible, can overfit\n",
    "- **Large k** (e.g., k=50): Model is simple, might underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a KNN pipeline with StandardScaler and KNeighborsClassifier\n",
    "# Use make_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29fd0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a validation curve for the KNN pipeline\n",
    "# - Define k_range using np.arange(1, 31, 1)\n",
    "# - Use ValidationCurveDisplay.from_estimator() with:\n",
    "#   - param_name=\"kneighborsclassifier__n_neighbors\"\n",
    "#   - param_range=k_range\n",
    "#   - cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#   - scoring=\"accuracy\"\n",
    "#   - n_jobs=-1\n",
    "# - Set appropriate title and labels\n",
    "# - Display the plot with plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afbc9e1",
   "metadata": {},
   "source": [
    "**Question 1:** Looking at the validation curve:\n",
    "- What happens with small k values (1-5)?\n",
    "- What is the optimal k range?\n",
    "- What happens with large k values (>15)?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d54176",
   "metadata": {},
   "source": [
    "### 2.2 Validation Curve for Random Forest (max_depth parameter)\n",
    "\n",
    "In Random Forest, the `max_depth` parameter controls tree depth:\n",
    "- **Small max_depth**: Trees are shallow and simple (underfitting risk)\n",
    "- **Large max_depth**: Trees are deep and complex (overfitting risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf13903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a Random Forest classifier\n",
    "# Use: RandomForestClassifier with n_estimators=100, min_samples_split=10, \n",
    "#      min_samples_leaf=4, random_state=42\n",
    "# Note: min_samples_split and min_samples_leaf prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a14add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a validation curve for Random Forest\n",
    "# - Define depth_range using np.arange(1, 21, 1)\n",
    "# - Use ValidationCurveDisplay.from_estimator() with param_name=\"max_depth\"\n",
    "# - Use the same cv and scoring as before\n",
    "# - Set appropriate title and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466006e",
   "metadata": {},
   "source": [
    "**Question 2:** Looking at the validation curve:\n",
    "- What is the effect of small max_depth values?\n",
    "- What is the optimal max_depth range?\n",
    "- Why don't we see perfect 1.0 training accuracy?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4f577",
   "metadata": {},
   "source": [
    "### 2.3 Validation Curve for SVM (gamma parameter)\n",
    "\n",
    "In SVM with RBF kernel, the `gamma` parameter controls how much each training point influences the decision boundary:\n",
    "- **Small gamma**: Smooth decision boundary (simple model)\n",
    "- **Large gamma**: Complex, wiggly decision boundary (complex model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create an SVM pipeline with StandardScaler and SVC\n",
    "# Use SVC with kernel=\"rbf\" and random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a validation curve for SVM\n",
    "# - Define gamma_range using np.logspace(-3, 1, num=20) for logarithmic scale\n",
    "# - Use ValidationCurveDisplay.from_estimator() with param_name=\"svc__gamma\"\n",
    "# - Use the same cv and scoring as before\n",
    "# - Set appropriate title (mention logarithmic scale) and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8e178",
   "metadata": {},
   "source": [
    "**Question 3:** Looking at the validation curve:\n",
    "- What is the effect of small gamma values?\n",
    "- What is the optimal gamma range?\n",
    "- What happens with large gamma values?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96427537",
   "metadata": {},
   "source": [
    "## 3. Understanding Learning Curves\n",
    "\n",
    "A **learning curve** shows how a model's performance improves as we increase the amount of training data.\n",
    "\n",
    "The curve plots:\n",
    "- **Training score**: Performance on the training data (typically high and stable)\n",
    "- **Validation score**: Performance on validation folds (typically improves with more data)\n",
    "\n",
    "Different patterns tell us:\n",
    "1. **Converging curves (small gap)**: Model has high bias, adding more data won't help much\n",
    "2. **Diverging curves (large gap)**: Model has high variance, adding more data will help\n",
    "3. **Both curves plateau early**: Model is likely underfitting (need more complex model)\n",
    "4. **Validation curve still rising**: Collecting more data would help (high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae237b97",
   "metadata": {},
   "source": [
    "### 3.1 Learning Curve for KNN (with optimal k from validation curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a KNN pipeline with optimal k=5 (from validation curve)\n",
    "# Use make_pipeline with StandardScaler and KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcf870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a learning curve for KNN\n",
    "# - Use LearningCurveDisplay.from_estimator()\n",
    "# - Use cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# - Use scoring=\"accuracy\"\n",
    "# - Use train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "# - Use n_jobs=-1\n",
    "# - Set appropriate title and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f56eed",
   "metadata": {},
   "source": [
    "**Question 4:** Looking at the learning curve:\n",
    "- What happens to the training score as we add more data?\n",
    "- What happens to the validation score as we add more data?\n",
    "- Would collecting more data help this model?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dd18f",
   "metadata": {},
   "source": [
    "### 3.2 Learning Curve for Random Forest (with optimal depth from validation curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebacc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a Random Forest with optimal parameters\n",
    "# Use: RandomForestClassifier with n_estimators=100, max_depth=8,\n",
    "#      min_samples_split=10, min_samples_leaf=4, random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec33e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a learning curve for Random Forest\n",
    "# Use the same parameters as for KNN above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e256b4",
   "metadata": {},
   "source": [
    "**Question 5:** Compare the gap between training and validation scores.\n",
    "- Is the model well-tuned?\n",
    "- Would more data help?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc7512",
   "metadata": {},
   "source": [
    "### 3.3 Learning Curve for SVM (with optimal gamma from validation curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fa86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create an SVM pipeline with optimal gamma=0.01\n",
    "# Use make_pipeline with StandardScaler and SVC(kernel=\"rbf\", gamma=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a learning curve for SVM\n",
    "# Use the same parameters as before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f4eea",
   "metadata": {},
   "source": [
    "**Question 6:** Observe the validation curve at the right end.\n",
    "- Is it still rising or has it plateaued?\n",
    "- What does this tell us about data collection?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71443565",
   "metadata": {},
   "source": [
    "## 4. Practical Decision-Making Guide\n",
    "\n",
    "### How to use these curves to make decisions:\n",
    "\n",
    "#### **Step 1: Use Validation Curves to Find Optimal Parameters**\n",
    "- Generate validation curves for key hyperparameters\n",
    "- Choose the parameter value where validation score is highest\n",
    "- Watch for signs of overfitting (large gap) or underfitting (both scores low)\n",
    "\n",
    "#### **Step 2: Use Learning Curves to Assess Data Needs**\n",
    "- Generate a learning curve with the optimal parameters\n",
    "- Look at the validation score trend:\n",
    "  - **Still rising at the end?** → Collect more data\n",
    "  - **Plateau?** → Either stop or use a more complex model\n",
    "  - **Gap closing as data increases?** → Good sign of improving generalization\n",
    "\n",
    "#### **Step 3: Interpret Different Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ef9c2",
   "metadata": {},
   "source": [
    "### 4.1 Example: Diagnosing a Poorly-Tuned Model (Underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a KNN pipeline with k=30 (too high, will underfit)\n",
    "# Then create a learning curve to see the underfitting pattern\n",
    "# What do you observe about both training and validation scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d40a8d",
   "metadata": {},
   "source": [
    "**Question 7:** Diagnosis of this model:\n",
    "- Are both training and validation scores high or low?\n",
    "- What is the gap between them?\n",
    "- What remedies would you suggest?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f714ef0",
   "metadata": {},
   "source": [
    "### 4.2 Example: Diagnosing an Overfitting Model (High Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a KNN pipeline with k=1 (too low, will overfit)\n",
    "# Then create a learning curve to see the overfitting pattern\n",
    "# What do you observe about the gap between training and validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c89e1",
   "metadata": {},
   "source": [
    "**Question 8:** Diagnosis of this model:\n",
    "- What is the training score?\n",
    "- What is the validation score?\n",
    "- What is the gap between them?\n",
    "- What remedies would you suggest?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed559a97",
   "metadata": {},
   "source": [
    "### 4.3 Example: Well-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a KNN pipeline with k=7 (well-balanced)\n",
    "# Then create a learning curve\n",
    "# How does this compare to the overfitting and underfitting examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb1053",
   "metadata": {},
   "source": [
    "**Question 9:** Characteristics of a well-tuned model:\n",
    "- What do you observe about the scores?\n",
    "- What is the gap between curves?\n",
    "- What would be your next steps?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a2465",
   "metadata": {},
   "source": [
    "## 5. Practical Exercise: Complete Analysis Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e5f6c",
   "metadata": {},
   "source": [
    "### 5.1 Compare multiple models using learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed59bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'KNN (k=5)': make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5)),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'SVM': make_pipeline(StandardScaler(), SVC(kernel='rbf', gamma=0.01, random_state=42))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete: Create a figure with 3 subplots (1 row, 3 columns)\n",
    "# For each model, create a learning curve in its subplot\n",
    "# Hint: Use plt.subplots(1, 3, figsize=(16, 5)) and iterate over models\n",
    "# Use enumerate() to get the index for the subplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576976f",
   "metadata": {},
   "source": [
    "**Question 10:** Compare the three models:\n",
    "- Which model has the best final validation score?\n",
    "- Which model has the smallest gap (best bias-variance tradeoff)?\n",
    "- Which model would you choose for production and why?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef872f",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Takeaways\n",
    "\n",
    "### Validation Curves:\n",
    "- **Purpose**: Find optimal hyperparameter values\n",
    "- **How to read**: Look for peak in validation score (orange line)\n",
    "- **Patterns**:\n",
    "  - Large gap: overfitting (high variance)\n",
    "  - Low scores both: underfitting (high bias)\n",
    "  - Tight curves at high score: well-tuned\n",
    "\n",
    "### Learning Curves:\n",
    "- **Purpose**: Determine if more data would help\n",
    "- **How to read**: Check if validation score still climbing\n",
    "- **Patterns**:\n",
    "  - Both curves plateau: enough data, focus on model complexity\n",
    "  - Validation still rising: more data would help\n",
    "  - Large gap: model has high variance, more data helps\n",
    "\n",
    "### Decision Tree:\n",
    "1. **Start with validation curve**: Find best hyperparameters\n",
    "2. **Create learning curve**: Check if you need more data\n",
    "3. **Interpret patterns**: Decide: more data? new model? more features?\n",
    "4. **Iterate**: Adjust model and retest\n",
    "\n",
    "### Key Metrics to Track:\n",
    "- Final validation score (absolute performance)\n",
    "- Gap between training and validation (overfitting indicator)\n",
    "- Slope of validation curve (data efficiency)\n",
    "- Whether curves have plateaued (saturation point)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
